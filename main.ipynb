{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'algo_p' from 'learning_tools' (c:\\Users\\Chardin Pierre\\OneDrive - CentraleSupelec\\Bureau\\CS\\3a\\DL\\RL-for-bridge-bidding\\learning_tools.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12920\\186237327.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnn_tools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mQfirst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQfollowing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimiers_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_epoch_layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mlearning_tools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpeak_one\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgo_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgo_e\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'algo_p' from 'learning_tools' (c:\\Users\\Chardin Pierre\\OneDrive - CentraleSupelec\\Bureau\\CS\\3a\\DL\\RL-for-bridge-bidding\\learning_tools.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm as tqdm\n",
    "import torch\n",
    "from nn_tools import Qfirst, Qfollowing, optimiers_list, one_epoch_layers\n",
    "import torch.nn as nn\n",
    "from learning_tools import peak_one, algo_p, algo_e, update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du modèle spécifique à chaque couche d'enchère\n",
    "def learning_algo(data,max_iter=100, nb_layers=5, epsilon=0.1,\n",
    "                  loss= nn.MSELoss(),batch_size=50, device='cpu'):\n",
    "\n",
    "    D = [[torch.empty(0), torch.empty(0)] for _ in range(nb_layers)]  # Un dataset D par couche\n",
    "    all_actions = list(range(36))  # 36 actions possibles\n",
    "    Q_models = [Qfirst() if i == 0 else Qfollowing() for i in range(nb_layers)]\n",
    "    losses=[]\n",
    "    for _ in tqdm(range(max_iter)):\n",
    "        bidding_history = np.zeros(36) # array de 5x36 pour enregistrer l'historique des enchères\n",
    "        x1, x2, scores = peak_one(data)\n",
    "\n",
    "        for j in range(nb_layers):\n",
    "            C = np.zeros(36)  \n",
    "            hand = x1 if (j+1) % 2 == 1 else x2  # main du joueur actuel\n",
    "\n",
    "            # Calcul des coûts des actions\n",
    "            for i, action in enumerate(all_actions):\n",
    "                C[i] = algo_p(action,x1,x2,scores,bidding_history,Q_models,nb_layers)\n",
    "            \n",
    "            # state = main si première couche sinon state = main + enchère_précédente\n",
    "            state = torch.tensor(hand, dtype=torch.float32) if j == 0 else torch.tensor(np.concatenate([hand, bidding_history]), dtype=torch.float32)\n",
    "            D=update(D,state,C,j) #ajout de l'expérience dans le dataset D_j\n",
    "            \n",
    "            next_a = algo_e(Q_models[j], state, epsilon, all_actions)  # Sélection avec Q_j\n",
    "            if next_a == len(bidding_history) - 1:  # si l'action est PASS\n",
    "                break  \n",
    "            bidding_history[next_a] = 1  # enregistrement de l'action choisie dans l'historique des enchères\n",
    "\n",
    "\n",
    "        # Entraînement des modèles Q\n",
    "        opti_list=optimiers_list(Q_models)\n",
    "        losses.append(one_epoch_layers(Q_models, D, opti_list,loss, batch_size, device))\n",
    "    return Q_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/slice0.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_algo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
