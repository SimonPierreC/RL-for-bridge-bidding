import torch.nn as nn
from torch.optim import RMSprop
import torch.nn.functional as F
from torch.nn import MSELoss
from torch import from_numpy
from torch.utils.data import DataLoader, TensorDataset
import numpy as np


class Qfirst(nn.Module):
    def __init__(self, input_dim=52, hidden_dim=128, output_dim=36):
        super(Qfirst, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, hidden_dim)
        self.fc4 = nn.Linear(hidden_dim, output_dim)
        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                m.weight.data.uniform_(-0.1, 0.1)
                m.bias.data.uniform_(-0.1, 0.1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        return self.fc4(x)


class Qfollowing(nn.Module):
    def __init__(self, input_dim=36+52, hidden_dim=128, output_dim=36):
        super(Qfollowing, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, hidden_dim)
        self.fc4 = nn.Linear(hidden_dim, output_dim)
        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                m.weight.data.uniform_(-0.1, 0.1)
                m.bias.data.uniform_(-0.1, 0.1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        return self.fc4(x)


def define_Q(number_of_layers):
    Q_list = []
    Q_list.append(Qfirst())
    for k in range(number_of_layers-1):
        Q_list.append(Qfollowing())
    return Q_list


def perform_one_epoch(model, train_loader, criterion, optimizer, device='cpu'):
    model.to(device)
    model.train()

    total_loss = 0.0
    for inputs, targets in train_loader:
        inputs, targets = inputs.to(device).float(), targets.to(device).float()

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
    return total_loss/len(train_loader)


def prepare_data(states, targets, batch_size):
    dataset = TensorDataset(states, targets)
    trainloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    return trainloader


def optimiers_list(layers_Q):
    return [RMSprop(model.parameters(), lr=0.83, alpha=0.98, momentum=0.82) for model in layers_Q]


def one_epoch_layers(layers_Q, D_by_layers, opti_list, loss=MSELoss(), batch_size=50, device="cpu"):
    """
    layers_Q: liste des modèles Q
    D_by_layer : jeu d'entraînement
                Format : iterable de taille len(layers_Q)
                Chaque élément D_by_layer[k] correspond au jeu d'entraînement pour la kème couche.
                Format de D_by_layer[k] : itérable de taille 2
                D_by_layer[k][0] est un tensor de shape (n_samples, n_state_features) contenant les états
                D_by_layer[k][1] est un tensor de shape (n_samples, 36) contenant les targets
    opti_list : list of optimizers. They can be generated by optimiers_list function
    """
    loss_by_layers = []
    for t, d_t in enumerate(D_by_layers):
        if d_t[1].shape[0] > 0:
            train_loader = prepare_data(d_t[0], d_t[1], batch_size)
            loss_by_layers.append(perform_one_epoch(
                layers_Q[t], train_loader, loss, opti_list[t], device))
    return loss_by_layers


def train_one_batch(model, train_loader, criterion, optimizer, device='cpu'):
    model.to(device)
    model.train()

    total_loss = 0.0
    for inputs, targets in train_loader:
        inputs, targets = inputs.to(device).float(), targets.to(device).float()

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        break
    return total_loss/len(targets)


def one_batch_layers(layers_Q, D_by_layers, opti_list, loss=MSELoss(), batch_size=50, device="cpu"):
    """
    layers_Q: liste des modèles Q
    D_by_layer : jeu d'entraînement
                Format : iterable de taille len(layers_Q)
                Chaque élément D_by_layer[k] correspond au jeu d'entraînement pour la kème couche.
                Format de D_by_layer[k] : itérable de taille 2
                D_by_layer[k][0] est un tensor de shape (n_samples, n_state_features) contenant les états
                D_by_layer[k][1] est un tensor de shape (n_samples, 36) contenant les targets
    opti_list : list of optimizers. They can be generated by optimiers_list function
    """
    loss_by_layers = []
    for t, d_t in enumerate(D_by_layers):
        if d_t[1].shape[0] > 0:
            train_loader = prepare_data(d_t[0], d_t[1], batch_size)
            loss_by_layers.append(train_one_batch(
                layers_Q[t], train_loader, loss, opti_list[t], device))
        else:
            loss_by_layers.append(np.nan)
    return loss_by_layers


class EarlyStopping():
    def __init__(self, min_delta, patience):
        self.best_weights = None
        self.best_loss = None
        self.loss = None
        self.epochs_no_impr = 0
        self.min_delta = min_delta
        self.patience = patience

    def test_stop(self, loss, weights):
        if self.loss is None:
            self.loss = loss
        if self.best_weights is None:
            self.best_weights = weights

        if self.loss - loss > self.min_delta:
            self.epochs_no_impr = 0
            self.best_weights, self.best_loss = weights, loss
            self.loss = loss
            return False
        if self.epochs_no_impr + 1 >= self.patience:
            return True
        self.epochs_no_impr += 1
        self.loss = loss
        return False

    def set_best_wieghts(self, model):
        model.load_state_dict(self.best_weights)


class EarlyStoppingList(EarlyStopping):
    def __init__(self, min_delta_list, patience_list):
        super().__init__(min_delta_list, patience_list)
        self.e_s_list = [EarlyStopping(min_delta_list[k], patience_list[k])
                         for k in range(len(min_delta_list))]

    def test_stop(self, loss_list, weights_list):
        output = True
        for i, e_s in enumerate(self.e_s_list):
            if not e_s.test_stop(loss_list[i], weights_list[i]):
                output = False
        return output

    def set_best_weights(self, models_list):
        for i, model in enumerate(models_list):
            self.e_s_list[i].set_best_wieghts(model)
