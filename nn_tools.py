import torch.nn as nn
from torch.optim import RMSprop
import torch.nn.functional as F
from torch.nn import MSELoss
from torch import from_numpy
from torch.utils.data import DataLoader, TensorDataset


class Qfirst(nn.Module):
    def __init__(self, input_dim=52, hidden_dim=128, output_dim=36):
        super(Qfirst, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, hidden_dim)
        self.fc4 = nn.Linear(hidden_dim, output_dim)
        self._init_weights()
        
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                m.weight.data.uniform_(-0.1, 0.1)
                m.bias.data.uniform_(-0.1, 0.1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        return self.fc4(x)


class Qfollowing(nn.Module):
    def __init__(self, input_dim=36+52, hidden_dim=128, output_dim=36):
        super(Qfollowing, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, hidden_dim)
        self.fc4 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        return self.fc4(x)


def define_Q(number_of_layers):
    Q_list = []
    Q_list.append(Qfirst())
    for k in range(number_of_layers-1):
        Q_list.append(Qfollowing())
    return Q_list


def perform_one_epoch(model, train_loader, criterion, optimizer, device='cpu'):
    model.to(device)
    model.train()

    total_loss = 0.0
    for inputs, targets in train_loader:
        inputs, targets = inputs.to(device).float(), targets.to(device).float()

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
    return total_loss/len(train_loader)


def prepare_data(states, targets, batch_size):
    dataset = TensorDataset(states, targets)
    trainloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    return trainloader


def optimiers_list(layers_Q):
    return [RMSprop(model.parameters(), lr=0.83, alpha=0.98, momentum=0.82) for model in layers_Q]


def one_epoch_layers(layers_Q, D_by_layers, opti_list, loss=MSELoss(), batch_size=50, device="cpu"):
    """
    layers_Q: liste des modèles Q
    D_by_layer : jeu d'entraînement
                Format : iterable de taille len(layers_Q)
                Chaque élément D_by_layer[k] correspond au jeu d'entraînement pour la kème couche.
                Format de D_by_layer[k] : itérable de taille 2
                D_by_layer[k][0] est un tensor de shape (n_samples, n_state_features) contenant les états
                D_by_layer[k][1] est un tensor de shape (n_samples, 36) contenant les targets
    opti_list : list of optimizers. They can be generated by optimiers_list function
    """
    loss_by_layers = []
    for t, d_t in enumerate(D_by_layers):
        train_loader = prepare_data(d_t[0], d_t[1], batch_size)
        loss_by_layers.append(perform_one_epoch(
            layers_Q[t], train_loader, loss, opti_list[t], device))
    return loss_by_layers
