{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algo espilon greedy (exploration vs exploitation) \n",
    "def algo_e(Q,state,epsilon,all_actions):\n",
    "    if np.random.rand() < epsilon:\n",
    "        action=np.random.choice(all_actions) #random action\n",
    "    else:\n",
    "        action=np.argmax(Q[state]) #with the highest q_value\n",
    "    return action\n",
    "\n",
    "#to change for UCB algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 176 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9  ...   166   167   168  \\\n",
       "0  1.0  0.0  1.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.64  0.64  0.68   \n",
       "1  0.0  0.0  0.0  0.0  1.0  1.0  0.0  1.0  0.0  0.0  ...  0.60  0.68  0.64   \n",
       "2  0.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...  0.56  0.56  0.68   \n",
       "3  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  ...  0.52  0.44  0.44   \n",
       "4  0.0  0.0  1.0  0.0  1.0  0.0  1.0  1.0  0.0  0.0  ...  0.68  0.56  0.52   \n",
       "5  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.60  0.60  0.60   \n",
       "6  0.0  1.0  0.0  1.0  1.0  1.0  0.0  0.0  1.0  0.0  ...  0.56  0.64  0.68   \n",
       "7  0.0  0.0  1.0  0.0  1.0  1.0  0.0  1.0  0.0  0.0  ...  0.72  0.60  0.48   \n",
       "8  1.0  1.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  0.48  0.68  0.60   \n",
       "9  0.0  1.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0  ...  0.60  0.56  0.52   \n",
       "\n",
       "    169   170   171   172   173   174   175  \n",
       "0  0.56  0.60  0.60  0.60  0.64  0.52  0.56  \n",
       "1  0.64  0.64  0.56  0.64  0.60  0.60  0.60  \n",
       "2  0.64  0.60  0.52  0.52  0.64  0.60  0.56  \n",
       "3  0.52  0.48  0.48  0.44  0.44  0.52  0.48  \n",
       "4  0.56  0.56  0.64  0.52  0.48  0.52  0.52  \n",
       "5  0.56  0.56  0.56  0.56  0.56  0.52  0.56  \n",
       "6  0.48  0.48  0.56  0.60  0.64  0.48  0.48  \n",
       "7  0.56  0.48  0.68  0.56  0.48  0.52  0.48  \n",
       "8  0.48  0.48  0.48  0.64  0.56  0.48  0.48  \n",
       "9  0.64  0.56  0.56  0.52  0.52  0.60  0.52  \n",
       "\n",
       "[10 rows x 176 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(f\"data/slice{0}.csv\", index_col = 0)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0.]\n",
      "x2: [0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1.]\n",
      "c: [0.88 0.8  1.   0.96 0.88 0.88 0.76 0.84 0.84 0.84 0.84 0.72 0.8  0.76\n",
      " 0.76 0.76 0.68 0.76 0.72 0.72 0.72 0.64 0.68 0.68 0.68 0.68 0.6  0.64\n",
      " 0.64 0.64 0.64 0.56 0.6  0.6  0.6  0.6 ]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "def peak_one(data):\n",
    "    row = data.sample(n=1).iloc[0]\n",
    "    x1 = row[:52].values\n",
    "    x2 = row[52:104].values\n",
    "    c = row[104:].values\n",
    "    return x1, x2, c\n",
    "\n",
    "#example\n",
    "x1, x2, c = peak_one(data)\n",
    "print(\"x1:\", x1)\n",
    "print(\"x2:\", x2)\n",
    "print(\"c:\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 shape: (52, 52, 36)\n",
      "Q2 shape: (52, 52, 36)\n",
      "Q3 shape: (52, 52, 36)\n",
      "Q4 shape: (52, 52, 36)\n",
      "Q5 shape: (52, 52, 36)\n"
     ]
    }
   ],
   "source": [
    "# Number of layers\n",
    "l = 5\n",
    "\n",
    "# Initialize action-value function Qj with random weights for j = 1, ..., l\n",
    "Q = [np.random.rand(52, 52, 36) for _ in range(l)]\n",
    "\n",
    "# Print the shape of Qj to verify\n",
    "for j in range(l):\n",
    "    print(f\"Q{j+1} shape: {Q[j].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "j=3\n",
    "hand = 0 if j % 2 == 1 else 0  \n",
    "print(hand)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_actions=list(range(36))\n",
    "nb_layers=5\n",
    "#state=[hand,history_bidding]\n",
    "\n",
    "def learning_algo(data,nb_layers=5,epsilon=0.1):\n",
    "  D=[]\n",
    "  Q_models = [np.random.rand(52, 52, 36) for _ in range(nb_layers)]\n",
    "  \n",
    "  #repeat avec early stopping\n",
    "    layers_explored=0\n",
    "    bidding_history=[]\n",
    "    x1,x2,scores=peak_one(data)\n",
    "\n",
    "    for j in range(nb_layers):\n",
    "      C=np.zeros(36)\n",
    "\n",
    "      if j%2==1:\n",
    "        hand=x1\n",
    "      else:\n",
    "        hand=x2\n",
    "\n",
    "      for i,action in enumerate(all_actions):\n",
    "        C[i]=algo_p(action,scores)\n",
    "\n",
    "      D.append([(hand,bidding_history.copy()),C])\n",
    "      \n",
    "      state = (np.argmax(hand), np.argmax(bidding_history) if bidding_history else 0)\n",
    "\n",
    "      next_a=algo_e(Q_models[j],state,epsilon,all_actions)\n",
    "\n",
    "      if next_a==\"PASS\":\n",
    "        layers_explored=j\n",
    "        break\n",
    "      bidding_history.append(next_a)\n",
    "\n",
    "    for i in range(np.max(layers_explored,nb_layers)):\n",
    "      #a voir si minibatch ou pas, peut commencer sans\n",
    "      train(Q,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(Q, D, learning_rate=0.01, gamma=0.9):\n",
    "    for (state, C) in D:\n",
    "        hand, history = state\n",
    "        state_idx = (np.argmax(hand), np.argmax(history) if history else 0)\n",
    "        \n",
    "        for action, reward in enumerate(C):\n",
    "            target = reward + gamma * np.max(Q[state_idx])  # Mise à jour des Q-values\n",
    "            Q[state_idx[0], state_idx[1], action] += learning_rate * (target - Q[state_idx[0], state_idx[1], action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nn_tools import Qfirst, Qfollowing, one_epoch_layers,optimiers_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 1s in bidding_history: 3\n"
     ]
    }
   ],
   "source": [
    "bidding_history=np.zeros(36)\n",
    "bidding_history[14:17]=1\n",
    "num_ones = np.count_nonzero(bidding_history == 1)\n",
    "print(f\"Number of 1s in bidding_history: {num_ones}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algo_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brouillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Déterminer le coût d'une action en fonction des scores\n",
    "def algo_p(action,x1,x2,bidding_history,scores,Q_models):\n",
    "    num_ones = np.count_nonzero(bidding_history == 1)\n",
    "    for i in range(num_ones,nb_layers):\n",
    "        if i % 2 == 1:\n",
    "            hand = x1\n",
    "        else:\n",
    "            hand = x2\n",
    "        state=hand if i==0 else (hand,bidding_history)\n",
    "\n",
    "        next_a=np.argmax(Q_models[i][state][action]) \n",
    "        bidding_history[next_a]=1\n",
    "\n",
    "        if next_a==len(bidding_history)-1 or i==nb_layers-1:\n",
    "            last_action=next_a\n",
    "\n",
    "        action=next_a\n",
    "\n",
    "    return scores[last_action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def algo_p(action, x1, x2, bidding_history, scores, Q_models, nb_layers):\n",
    "    num_ones = np.count_nonzero(bidding_history == 1)\n",
    "\n",
    "    for i in range(num_ones, nb_layers):\n",
    "        hand = x1 if i % 2 == 1 else x2  # Détermine la main en fonction du joueur actif\n",
    "        state = torch.tensor(hand, dtype=torch.float32) if i == 0 else torch.tensor(np.concatenate([hand, bidding_history]), dtype=torch.float32)\n",
    "\n",
    "        # Passer l'état dans le modèle Q et choisir l'action avec la plus grande valeur Q\n",
    "        with torch.no_grad():\n",
    "            q_values = Q_models[i](state)  # Prédiction du modèle Q\n",
    "            next_a = torch.argmax(q_values).item()  # Récupérer l'action avec le Q maximal\n",
    "\n",
    "        bidding_history[next_a] = 1  # Met à jour l'historique des enchères\n",
    "\n",
    "        if next_a == len(bidding_history) - 1 or i == nb_layers - 1:\n",
    "            last_action = next_a  # Sauvegarde de la dernière action valide\n",
    "\n",
    "        action = next_a  # Mise à jour de l'action pour la prochaine itération\n",
    "\n",
    "    return scores[last_action]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def perform_bids(x1, x2, scores,bidding_history, Q_models, nb_layers):\n",
    "    num_ones = np.count_nonzero(bidding_history == 1)\n",
    "\n",
    "    for i in range(num_ones, nb_layers):\n",
    "        hand = x1 if (i+1) % 2 == 1 else x2  # Détermine la main en fonction du joueur actif\n",
    "        \n",
    "        state = torch.tensor(hand, dtype=torch.float32) if i == 0 else torch.tensor(np.concatenate([hand, bidding_history]), dtype=torch.float32)\n",
    "\n",
    "        # passer l'état dans le modèle Q[i] et obtenir les valeurs Q[i]\n",
    "        with torch.no_grad():\n",
    "            q_values = Q_models[i](state)  # prédiction du modèle Q[i] pour le state\n",
    "\n",
    "            next_a = torch.argmax(q_values).item()  # sélection greedy max(Q)   a mettre la regle next_a>derniere enchere\n",
    "\n",
    "        bidding_history[next_a] = 1  # met à jour l'historique des enchères\n",
    "\n",
    "        if next_a == len(bidding_history) - 1 or i == nb_layers - 1:\n",
    "            last_action = next_a  # sauvegarde de la dernière action valide\n",
    " \n",
    "    return scores[last_action]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand: [0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0.\n",
      " 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "state: tensor([0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "next_a: 5\n",
      "bidding_history: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "hand: [1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 0. 0.]\n",
      "state: tensor([1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "next_a: 29\n",
      "bidding_history: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "hand: [0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0.\n",
      " 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "state: tensor([0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "next_a: 26\n",
      "bidding_history: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "hand: [1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 0. 0.]\n",
      "state: tensor([1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "next_a: 13\n",
      "bidding_history: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "last_action: 13\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.76)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Q_models = [Qfirst() if i == 0 else Qfollowing() for i in range(nb_layers)]\n",
    "\n",
    "perform_bids(x1, x2, c, bidding_history, Q_models, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algo_p(action, x1, x2, scores,bidding_history, Q_models, nb_layers):\n",
    "    updated_bid_history = bidding_history.copy()\n",
    "    updated_bid_history[action] = 1\n",
    "    if np.count_nonzero(updated_bid_history) == nb_layers:\n",
    "        return scores[action]\n",
    "    final_score=perform_bids(x1, x2,scores, updated_bid_history, Q_models, nb_layers)\n",
    "    return final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0.\n",
      " 1. 0. 0. 0.]\n",
      "x2: [0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1.\n",
      " 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "scores: [0.84 0.84 0.96 1.   0.88 0.92 0.8  0.96 1.   0.76 0.8  0.72 0.96 0.84\n",
      " 0.72 0.76 0.68 0.76 0.76 0.68 0.68 0.64 0.72 0.72 0.64 0.64 0.6  0.68\n",
      " 0.68 0.6  0.6  0.56 0.64 0.64 0.56 0.6 ]\n",
      "hand: [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0.\n",
      " 1. 0. 0. 0.]\n",
      "state: tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "next_a: 15\n",
      "bidding_history: [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "hand: [0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1.\n",
      " 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "next_a: 13\n",
      "bidding_history: [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "hand: [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0.\n",
      " 1. 0. 0. 0.]\n",
      "state: tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "next_a: 22\n",
      "bidding_history: [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "last_action: 22\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.72)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example\n",
    "x1, x2, scores = peak_one(data)\n",
    "print(\"x1:\", x1)\n",
    "print(\"x2:\", x2)\n",
    "print(\"scores:\", scores)\n",
    "bidding_history=np.zeros(36)\n",
    "bidding_history[5]=1\n",
    "action=6\n",
    "\n",
    "nb_layers = 5\n",
    "Q_models = [Qfirst() if i == 0 else Qfollowing() for i in range(nb_layers)]\n",
    "\n",
    "algo_p(action,x1,x2,scores,bidding_history,Q_models,nb_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algo_E et peak_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratégie epsilon-greedy pour explorer/exploiter les actions\n",
    "def algo_e(Q, state, epsilon, all_actions):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(all_actions)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            print(Q(state))\n",
    "            return np.argmax(Q(state))  \n",
    "\n",
    "# Sélectionne une ligne aléatoire du dataset et extrait les informations\n",
    "def peak_one(data):\n",
    "    row = data.sample(n=1).iloc[0]  # Sélection aléatoire d’une ligne\n",
    "    x1 = row[:52].values  # Main du joueur 1\n",
    "    x2 = row[52:104].values  # Main du joueur 2\n",
    "    scores = row[104:140].values  # Scores des 36 actions possibles joueur\n",
    "    return x1, x2, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: [0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 1. 0. 0.]\n",
      "x2: [0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0.]\n",
      "c1: [1.   0.68 0.68 0.76 0.72 0.68 0.64 0.64 0.72 0.68 0.64 0.6  0.6  0.68\n",
      " 0.64 0.6  0.6  0.56 0.64 0.6  0.56 0.56 0.56 0.6  0.56 0.56 0.52 0.52\n",
      " 0.56 0.52 0.52 0.52 0.52 0.56 0.52 0.52]\n",
      "c2: [1.   0.68 0.68 0.76 0.72 0.68 0.64 0.64 0.72 0.68 0.64 0.6  0.6  0.68\n",
      " 0.64 0.6  0.6  0.56 0.64 0.6  0.56 0.56 0.56 0.6  0.56 0.56 0.52 0.52\n",
      " 0.56 0.52 0.52 0.52 0.52 0.56 0.52 0.52]\n"
     ]
    }
   ],
   "source": [
    "x1,x2,c1,c2=peak_one(data)\n",
    "print(\"x1:\", x1)\n",
    "print(\"x2:\", x2)\n",
    "print(\"c1:\", c1)    \n",
    "print(\"c2:\", c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(D, state, C, j):\n",
    "    D[j][0] = torch.concat([D[j][0], state.clone().detach().unsqueeze(0)])\n",
    "    D[j][1] = torch.concat([D[j][1], torch.tensor(C, dtype=torch.float32).unsqueeze(0)])\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du modèle spécifique à chaque couche d'enchère\n",
    "def learning_algo(data,max_iter=100, nb_layers=5, epsilon=0.1,\n",
    "                  loss= nn.MSELoss(),batch_size=50, device='cpu'):\n",
    "\n",
    "    D = [[torch.empty(0), torch.empty(0)] for _ in range(nb_layers)]  # Un dataset D par couche\n",
    "    all_actions = list(range(36))  # 36 actions possibles\n",
    "    Q_models = [Qfirst() if i == 0 else Qfollowing() for i in range(nb_layers)]\n",
    "    losses=[]\n",
    "    for _ in range(max_iter):\n",
    "        bidding_history = np.zeros(36) # array de 5x36 pour enregistrer l'historique des enchères\n",
    "        x1, x2, scores = peak_one(data)\n",
    "\n",
    "        for j in range(nb_layers):\n",
    "            C = np.zeros(36)  \n",
    "            hand = x1 if (j+1) % 2 == 1 else x2  # main du joueur actuel\n",
    "\n",
    "            # Calcul des coûts des actions\n",
    "            for i, action in enumerate(all_actions):\n",
    "                C[i] = algo_p(action,x1,x2,scores,bidding_history,Q_models,nb_layers)\n",
    "            \n",
    "            # state = main si première couche sinon state = main + enchère_précédente\n",
    "            state = torch.tensor(hand, dtype=torch.float32) if j == 0 else torch.tensor(np.concatenate([hand, bidding_history]), dtype=torch.float32)\n",
    "\n",
    "            D=update(D,state,C,j) #ajout de l'expérience dans le dataset D_j\n",
    "            \n",
    "            next_a = algo_e(Q_models[j], state, epsilon, all_actions)  # Sélection avec Q_j\n",
    "            \n",
    "            if next_a == len(bidding_history) - 1:  # si l'action est PASS\n",
    "                break  \n",
    "            \n",
    "            bidding_history[next_a] = 1  # enregistrement de l'action choisie dans l'historique des enchères\n",
    "\n",
    "\n",
    "        # Entraînement des modèles Q\n",
    "        opti_list=optimiers_list(Q_models)\n",
    "        losses.append(one_epoch_layers(Q_models, D, opti_list,loss, batch_size, device))\n",
    "    return Q_models\n",
    "    \n",
    "\n",
    "# Prendre l'entrainement de Pierrot\n",
    "# Fonction d'entraînement pour chaque modèle Q\n",
    "# def train(Q, D, learning_rate=0.01, gamma=0.9):\n",
    "#     for state, C in D:\n",
    "#         for action, reward in enumerate(C):\n",
    "#             target = reward + gamma * np.max(Q[state])  # Mise à jour de Bellman\n",
    "#             Q[state[0], state[1], action] += learning_rate * (target - Q[state[0], state[1], action])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0612,  0.0036,  0.1176,  0.0202,  0.0113, -0.0168,  0.0635,  0.1179,\n",
      "         0.0176,  0.0607,  0.0193,  0.0267, -0.0052, -0.0171,  0.0397,  0.0137,\n",
      "        -0.0374,  0.0962,  0.0575,  0.0007, -0.0104, -0.0762,  0.0633,  0.0479,\n",
      "         0.0151, -0.0654, -0.0236, -0.0588,  0.0456, -0.0546, -0.0834,  0.0248,\n",
      "        -0.0857, -0.1218,  0.0488, -0.1065])\n",
      "tensor([-7.7297e-02,  5.3583e-02, -8.6979e-02, -3.0479e-02, -7.9605e-02,\n",
      "        -2.0148e-02, -6.2806e-02, -4.6856e-02,  4.0937e-02,  2.3690e-02,\n",
      "        -8.2505e-02,  6.7428e-02,  1.2974e-02,  1.5879e-02, -6.0959e-02,\n",
      "        -1.2993e-02,  8.4731e-02, -3.4982e-02, -8.5863e-02,  6.8263e-02,\n",
      "        -2.3658e-02, -4.6052e-02, -9.1390e-03,  2.9613e-02, -2.3540e-02,\n",
      "         1.4946e-03, -9.5212e-03, -3.2355e-02, -1.6635e-03,  6.4065e-02,\n",
      "        -2.7838e-02,  5.6802e-02, -5.9784e-02, -4.1441e-02, -5.6408e-02,\n",
      "        -3.6731e-06])\n",
      "tensor([-0.1229,  0.0887,  0.0391,  0.0598,  0.0187,  0.1222, -0.0415, -0.0509,\n",
      "         0.0691, -0.0754,  0.0450, -0.0066,  0.0883,  0.0213, -0.0356, -0.1042,\n",
      "        -0.0233,  0.0261, -0.0515, -0.0561,  0.0216, -0.0496, -0.0092, -0.0198,\n",
      "        -0.0590,  0.0842,  0.0155, -0.0882, -0.0223, -0.0431,  0.0576, -0.0580,\n",
      "         0.0437, -0.0363,  0.0708,  0.0657])\n",
      "tensor([-8.5566e-02,  6.1941e-02, -7.8407e-02,  8.1513e-02,  6.1275e-03,\n",
      "         7.1086e-02,  3.6798e-02,  5.8029e-02,  7.1172e-02, -3.2322e-02,\n",
      "        -7.0687e-02,  1.3914e-01,  2.3269e-03,  6.4264e-02, -8.9467e-03,\n",
      "        -1.3053e-02, -1.1191e-01,  3.1259e-03,  2.1629e-02, -4.3873e-02,\n",
      "         5.9693e-02, -1.3727e-04, -3.0897e-02,  8.5215e-02,  8.8934e-02,\n",
      "        -8.9994e-02,  7.3792e-02,  4.4655e-03, -4.3612e-02, -8.2505e-02,\n",
      "        -5.2288e-02,  1.0867e-01,  1.3859e-02,  6.9832e-02,  4.2671e-02,\n",
      "        -1.9689e-02])\n",
      "tensor([-0.0419, -0.0717, -0.0939,  0.1376, -0.0447, -0.1335,  0.0436, -0.0953,\n",
      "         0.0850,  0.0908,  0.0007, -0.0272,  0.0663, -0.0160,  0.0494, -0.0067,\n",
      "         0.1399, -0.0417,  0.0042, -0.0845,  0.0074, -0.0391, -0.0314,  0.0088,\n",
      "         0.0269,  0.0579,  0.0173, -0.0014, -0.0598,  0.0254,  0.0542, -0.1290,\n",
      "        -0.0784, -0.0233,  0.0538, -0.0566])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torch._dynamo' has no attribute 'external_utils' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[169], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlearning_algo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMSELoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[168], line 35\u001b[0m, in \u001b[0;36mlearning_algo\u001b[1;34m(data, max_iter, nb_layers, epsilon, loss, batch_size, device)\u001b[0m\n\u001b[0;32m     31\u001b[0m         bidding_history[next_a] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# enregistrement de l'action choisie dans l'historique des enchères\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Entraînement des modèles Q\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m     opti_list\u001b[38;5;241m=\u001b[39m\u001b[43moptimiers_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ_models\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(one_epoch_layers(Q_models, D, opti_list,loss, batch_size, device))\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Q_models\n",
      "File \u001b[1;32mc:\\Users\\admin\\Desktop\\Projet Reinforcement Deep\\RL-for-bridge-bidding\\nn_tools.py:72\u001b[0m, in \u001b[0;36moptimiers_list\u001b[1;34m(layers_Q)\u001b[0m\n\u001b[0;32m     70\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     71\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m---> 72\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     73\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     75\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\admin\\Desktop\\Projet Reinforcement Deep\\RL-for-bridge-bidding\\nn_tools.py:72\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     70\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     71\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m---> 72\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     73\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     75\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\rmsprop.py:68\u001b[0m, in \u001b[0;36mRMSprop.__init__\u001b[1;34m(self, params, lr, alpha, eps, weight_decay, momentum, centered, capturable, foreach, maximize, differentiable)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid alpha value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malpha\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     56\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m     57\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m     58\u001b[0m     momentum\u001b[38;5;241m=\u001b[39mmomentum,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[0;32m     67\u001b[0m )\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\optimizer.py:371\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    368\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: param_groups}]\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[1;32m--> 371\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warned_capturable_if_run_uncaptured \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_compile.py:27\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m disable_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dynamo_disable\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m disable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[0;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_dynamo\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_frame, eval_frame, resume_execution\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py:53\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_python_dispatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     48\u001b[0m     _disable_current_modes,\n\u001b[0;32m     49\u001b[0m     is_in_torch_dispatch_mode,\n\u001b[0;32m     50\u001b[0m )\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_traceback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CapturedTraceback, format_traceback_short\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config, exc, trace_rules\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbytecode_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m remove_dead_code, remove_pointless_jumps\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbytecode_transformation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     56\u001b[0m     check_inst_exn_tab_entries_valid,\n\u001b[0;32m     57\u001b[0m     Instruction,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m     transform_code_object,\n\u001b[0;32m     61\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_dynamo\\trace_rules.py:46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresume_execution\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TORCH_DYNAMO_RESUME_IN_PREFIX\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getfile, hashable, NP_SUPPORTED_MODULES, unwrap_if_wrapper\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvariables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     47\u001b[0m     BuiltinVariable,\n\u001b[0;32m     48\u001b[0m     FunctionalCallVariable,\n\u001b[0;32m     49\u001b[0m     FunctorchHigherOrderVariable,\n\u001b[0;32m     50\u001b[0m     NestedUserFunctionVariable,\n\u001b[0;32m     51\u001b[0m     PolyfilledFunctionVariable,\n\u001b[0;32m     52\u001b[0m     SkipFunctionVariable,\n\u001b[0;32m     53\u001b[0m     TorchInGraphFunctionVariable,\n\u001b[0;32m     54\u001b[0m     UserFunctionVariable,\n\u001b[0;32m     55\u001b[0m     UserMethodVariable,\n\u001b[0;32m     56\u001b[0m )\n\u001b[0;32m     59\u001b[0m np: Optional[types\u001b[38;5;241m.\u001b[39mModuleType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_dynamo\\variables\\__init__.py:104\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdpa\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SDPAParamsVariable\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     97\u001b[0m     FakeItemVariable,\n\u001b[0;32m     98\u001b[0m     NumpyNdarrayVariable,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m     UntypedStorageVariable,\n\u001b[0;32m    103\u001b[0m )\n\u001b[1;32m--> 104\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TorchCtxManagerClassVariable, TorchInGraphFunctionVariable\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01muser_defined\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    106\u001b[0m     MutableMappingVariable,\n\u001b[0;32m    107\u001b[0m     RemovableHandleVariable,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    110\u001b[0m     WeakRefVariable,\n\u001b[0;32m    111\u001b[0m )\n\u001b[0;32m    114\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutogradFunctionContextVariable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutogradFunctionVariable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWithExitFunctionVariable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    179\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_dynamo\\variables\\torch.py:142\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Convert to dict for O(1) access times\u001b[39;00m\n\u001b[0;32m    133\u001b[0m constant_fold_functions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mfromkeys(constant_fold_functions)\n\u001b[0;32m    136\u001b[0m tracing_state_functions \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    137\u001b[0m     torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    138\u001b[0m     torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    139\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    140\u001b[0m     torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39m_symbolic_trace\u001b[38;5;241m.\u001b[39mis_fx_tracing: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    141\u001b[0m     torch\u001b[38;5;241m.\u001b[39monnx\u001b[38;5;241m.\u001b[39mis_in_onnx_export: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m--> 142\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexternal_utils\u001b[49m\u001b[38;5;241m.\u001b[39mis_compiling: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    143\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39mis_compiling: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    144\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcompiler\u001b[38;5;241m.\u001b[39mis_compiling: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    145\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcompiler\u001b[38;5;241m.\u001b[39mis_dynamo_compiling: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    146\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mactivation\u001b[38;5;241m.\u001b[39m_is_make_fx_tracing: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    147\u001b[0m }\n\u001b[0;32m    149\u001b[0m bin_ops \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mfromkeys([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msub\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmul\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqrt\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaseTorchVariable\u001b[39;00m(VariableTracker):\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'torch._dynamo' has no attribute 'external_utils' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "learning_algo(data,max_iter=2, nb_layers=5, epsilon=0.1,\n",
    "                  loss= nn.MSELoss(),batch_size=50, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### brouillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(C, dtype=torch.float32).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [1., 2., 3.]])\n",
      "tensor([[0.1000, 0.2000, 0.3000],\n",
      "        [0.1000, 0.2000, 0.3000]])\n"
     ]
    }
   ],
   "source": [
    "D = [[torch.empty(0), torch.empty(0)] for _ in range(5)]\n",
    "D = update(D, torch.tensor([1, 2, 3], dtype=torch.float32), [0.1, 0.2, 0.3], 0)\n",
    "D = update(D, torch.tensor([1, 2, 3], dtype=torch.float32), [0.1, 0.2, 0.3], 0)\n",
    "print(D[0][0])\n",
    "print(D[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test avec chat V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_dim, output_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(output_dim, activation='linear'))  # Q-value output for each action\n",
    "    model.compile(loss='mse', optimizer=Adam(lr=0.001))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the epsilon-greedy strategy for action selection\n",
    "def epsilon_greedy(model, state, epsilon, action_space):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(action_space)  # Choose a random action (exploration)\n",
    "    else:\n",
    "        q_values = model.predict(state)  # Predict Q-values for each action\n",
    "        return np.argmax(q_values[0])  # Choose the action with the highest Q-value (exploitation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_drl_model(data, model, gamma=0.9, epsilon=0.1, batch_size=64, epochs=10):\n",
    "    # data: [(x1, x2, contract_scores)] - where contract_scores is a vector of scores for possible actions\n",
    "    action_space = len(data[0][2])  # The number of possible actions (contracts)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        for i in range(len(data)):\n",
    "            x1, x2, contract_scores = data[i]\n",
    "            state = np.array([x1, x2]).reshape(1, -1)\n",
    "            target = model.predict(state)  # Get current Q-values for the state\n",
    "            \n",
    "            # Select the action (bid) using epsilon-greedy\n",
    "            action = epsilon_greedy(model, state, epsilon, range(action_space))\n",
    "            \n",
    "            # Compute the reward (contract score) for the chosen action\n",
    "            reward = contract_scores[action]  # Get the score for the chosen action\n",
    "            \n",
    "            # Update the Q-value using the Bellman equation\n",
    "            target[0][action] = reward + gamma * np.max(target[0])  # Bellman update\n",
    "            \n",
    "            # Train the model on this updated Q-value\n",
    "            model.fit(state, target, epochs=1, verbose=0)\n",
    "        \n",
    "        epsilon = max(0.01, epsilon * 0.995)  # Decay epsilon to reduce exploration over time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(f\"data/slice{0}.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9  ...   130   131   132  \\\n",
       "0  1.0  0.0  1.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.64  0.64  0.68   \n",
       "1  0.0  0.0  0.0  0.0  1.0  1.0  0.0  1.0  0.0  0.0  ...  0.60  0.68  0.64   \n",
       "2  0.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...  0.56  0.56  0.68   \n",
       "3  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  ...  0.52  0.44  0.44   \n",
       "4  0.0  0.0  1.0  0.0  1.0  0.0  1.0  1.0  0.0  0.0  ...  0.68  0.56  0.52   \n",
       "5  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.60  0.60  0.60   \n",
       "6  0.0  1.0  0.0  1.0  1.0  1.0  0.0  0.0  1.0  0.0  ...  0.56  0.64  0.68   \n",
       "7  0.0  0.0  1.0  0.0  1.0  1.0  0.0  1.0  0.0  0.0  ...  0.72  0.60  0.48   \n",
       "8  1.0  1.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  0.48  0.68  0.60   \n",
       "9  0.0  1.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0  ...  0.60  0.56  0.52   \n",
       "\n",
       "    133   134   135   136   137   138   139  \n",
       "0  0.56  0.60  0.60  0.60  0.64  0.52  0.56  \n",
       "1  0.64  0.64  0.56  0.64  0.60  0.60  0.60  \n",
       "2  0.64  0.60  0.52  0.52  0.64  0.60  0.56  \n",
       "3  0.52  0.48  0.48  0.44  0.44  0.52  0.48  \n",
       "4  0.56  0.56  0.64  0.52  0.48  0.52  0.52  \n",
       "5  0.56  0.56  0.56  0.56  0.56  0.52  0.56  \n",
       "6  0.48  0.48  0.56  0.60  0.64  0.48  0.48  \n",
       "7  0.56  0.48  0.68  0.56  0.48  0.52  0.48  \n",
       "8  0.48  0.48  0.48  0.64  0.56  0.48  0.48  \n",
       "9  0.64  0.56  0.56  0.52  0.52  0.60  0.52  \n",
       "\n",
       "[10 rows x 140 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data.iloc[:,:140]\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DRL model\n",
    "input_dim = 52  # Since each hand has 26 cards, so the total state dimension is 52\n",
    "output_dim = 4  # Assuming 4 possible actions (contracts)\n",
    "model = create_model(input_dim, output_dim)\n",
    "\n",
    "# Train the model\n",
    "train_drl_model(data, model, epochs=100)\n",
    "\n",
    "# Example: Testing the model on a new hand\n",
    "test_hand = np.random.rand(26)\n",
    "test_state = np.array([test_hand, test_hand]).reshape(1, -1)\n",
    "predicted_q_values = model.predict(test_state)\n",
    "predicted_bid = np.argmax(predicted_q_values[0])  # The predicted best contract (bid)\n",
    "print(f\"Predicted best contract bid: {predicted_bid}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
